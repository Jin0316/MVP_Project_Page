<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Instruction-Grounded Visual Projectors for Continual Learning of Generative Vision-Language Models</title>
  <link rel="icon" type="image/x-icon" href="static/images/VLLAB-02.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Instruction-Grounded Visual Projectors for Continual Learning of Generative Vision-Language Models</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://jin0316.github.io/HyundongJin.io/" target="_blank">Hyundong Jin</a><sup>1</sup>,</span>
                <span class="author-block">
                  <a href="https://www.birmingham.ac.uk/staff/profiles/computer-science/academic-staff/chang-jin-hyung" target="_blank">Hyung Jin Chang</a><sup>2</sup>,</span>
                  <span class="author-block">
                    <a href="https://vllab.cau.ac.kr/members/professor/" target="_blank">Eunwoo Kim<sup>1,*</sup></a>
                  </span>
                  </div>

                  <!-- Affiliations -->
                  <div class="is-size-6 publication-authors">
                    <div class="author-block">
                      <sup>1</sup> School of Computer Science and Engineering, Chung-Ang University, South Korea
                    </div>
                    <div class="author-block">
                      <sup>2</sup> School of Computer Science, University of Birmingham, United Kingdom
                    </div>
                  </div>
      
                  <!-- Footnotes on separate lines -->
                  <div class="is-size-6 has-text-grey">
                    <sup>*</sup> Corresponding Author
                  </div>
                  <div class="is-size-6 has-text-grey">
                    Accepted to ICCV 2025
                  </div>


                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span> 
                

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2508.00260" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Paper poster -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <figure class="has-text-centered">
        <img
          src="static/images/main figure.png"
          alt="Instruction-grounded mixture of visual projectors with expert selection conditioned on instruction semantics"
          style="max-width: 100%; height: auto;"
        />
        <figcaption class="subtitle" style="width: 100%; margin: 0.75rem auto 0;">
          <p>
           How can we achieve context-aware visual-to-language translation without neglecting text instructions after learning subsequent tasks?
          </p>
          <p>
           How can we achieve context-aware visual-to-language translation without neglecting text instructions after learning subsequent tasks?
          </p>
          <p>
           How can we achieve context-aware visual-to-language translation without neglecting text instructions after learning subsequent tasks?
          </p>
        </figcaption>
      </figure>
    </div>
  </div>
</section>
<!--End paper poster -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
          Continual learning enables pre-trained generative visionlanguage models (VLMs) to incorporate knowledge from new tasks without retraining data from previous ones. 
          Recent methods update a visual projector to translate visual information for new tasks, connecting pre-trained vision encoders with large language models. 
          However, such adjustments may cause the models to prioritize visual inputs over language instructions, particularly learning tasks with repetitive types of textual instructions. 
          To address the neglect of language instructions, we propose a novel framework that grounds the translation of visual information on instructions for language models. 
          We introduce a mixture of visual projectors, each serving as a specialized visual-tolanguage translation expert based on the given instruction context to adapt to new tasks. 
          To avoid using experts for irrelevant instruction contexts, we propose an expert recommendation strategy that reuses experts for tasks similar to those previously learned. 
          Additionally, we introduce expert pruning to alleviate interference from the use of experts that cumulatively activated in previous tasks. 
          Extensive experiments on diverse vision-language tasks demonstrate that our method outperforms existing continual learning approaches by generating instruction-following responses
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


  <!-- Image carousel -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <div id="results-carousel-1" class="carousel results-carousel">
          <div class="item">
            <img src="static/images/main results.png" alt="Instruction-aware projector selection across tasks"/>
            <h2 class="subtitle has-text-centered">Results of the compared methods that employed Vicuna were measured using the Last, Avg, and Transfer metrics.</h2>
          </div>
          <div class="item">
            <img src="static/images/addtional results.png" alt="Qualitative responses before and after sequential training"/>
            <h2 class="subtitle has-text-centered">Results of using VLM with LLaMa-2 (left) and  Ablation study of MVP with different components (right).</h2>
          </div>
          <div class="item">
            <img src="static/images/qualitative results.png" alt="Performance over time under different methods"/>
            <h2 class="subtitle has-text-centered">Qualitative results of the continual learning methods illustrating changes of responses to randomly selected samples after learning of newly emerging tasks.</h2>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- /Image carousel -->







<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
